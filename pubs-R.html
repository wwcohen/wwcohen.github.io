<html><head><title>William W. Cohen's Papers: Retrieval Augmented LMs</title>
<link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body><h3>William W. Cohen's Papers: Retrieval Augmented LMs</h3>
<ol>
<li>Tal Schuster, Adam D. Lelkes, Haitian Sun, Jai Gupta, Jonathan Berant, William W. Cohen, Donald Metzler (2024): <a href="https://arxiv.org/abs/2311.04886">SEMQA: Semi-Extractive Multi-Source Question Answering</a> in NAACL-2024.
</li>
<li>Yury Zemlyanskiy, Michiel de Jong, Luke Vilnis, Santiago Ontañón, William W. Cohen, Sumit Sanghai, Joshua Ainslie (2024): <a href="https://arxiv.org/abs/2308.14903">MEMORY-VQ: Compression for Tractable Internet-Scale Memory</a> in NAACL-2024.
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2023): <a href="https://arxiv.org/abs/2308.08661">Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions</a> in progress.<br><ul><li><font size=-1>Following up the 'QA is the new KR' paper, we present a new collection of question-answer pairs automatically generated from Wikipedia which are more specific and ambiiguous than generated questions used in prior work, and show that this can be used to answer ambiguous questions.  On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 10-15%.  The new queston DB can also be used to improve diverse passage retrieval.</font></ul>
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Sumit Sanghai, William W. Cohen, Joshua Ainslie (2023): <a href="https://arxiv.org/abs/2306.10231">GLIMMER: generalized late-interaction memory reranker</a> in progress.
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, William W. Cohen (2023): <a href="https://arxiv.org/abs/2301.10448">Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute</a> in ICML-2023.
</li>
<li>Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen (2023): <a href="https://arxiv.org/abs/2210.02928">MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text</a> in EACL-2023.
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William Cohen (2023): <a href="https://arxiv.org/abs/2212.08153">FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference</a> in ACL-2023 (Findings).
</li>
<li>Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen (2023): <a href="https://arxiv.org/abs/2209.14491">Re-Imagen: Retrieval-Augmented Text-to-Image Generator</a> in ICLR-2023.
</li>
<li>Wenhu Chen, William W. Cohen, Michiel De Jong, Nitish Gupta, Alessandro Presta, Pat Verga, John Wieting (2023): <a href="https://arxiv.org/abs/2207.00630">QA Is the New KR: Question-Answer Pairs as Knowledge Bases</a> in AAAI-2023.<br><ul><li><font size=-1>Proposes that symbolic KBs can be replaced with a collection of question-answer pairs automatically generated from a corpus, augmented with entity-linking annotations.  Like a symbolic KB, this representation is well-suited to structured queries involving joins and aggregation, and can support 'multi-hop' reasoning.  However, it has the advantage that the information in it is closely aligned to likely user information needs, as modeled by the question generation process.</font></ul>
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2023): <a href="https://openreview.net/forum?id=tPrRs6YB2P">Scenario-based Question Answering with Interacting Contextual Properties</a> in ICLR-2023.
</li>
<li>Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster (2022): <a href="https://arxiv.org/abs/2212.08037">Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models</a> in progress.
</li>
<li>Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William W. Cohen (2022): <a href="https://arxiv.org/abs/2204.04581">Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering</a> in EACL-2022.<br><ul><li><font size=-1>Extends the techniques of Mention Memory in several important ways. (1) The memory is a memory of generated question-answer pairs, which is more interpretable than neural entity-mention encodings; (2) it is based on pre-trained T5, not a custom Transformer; and (3) it allows use of the token-level encoding of retrieved QA pairs as well as neural encodings of them for reasoning.  Using QA pairs instead of passages allows a clever pre-training trick for learning to retrieve, and the model greatly outperfoms a prior similar model (i.e., RePAQ) on smaller QA benchmarks.</font></ul>
</li>
<li>Vidhisha Balachandran and Bhuwan Dhingra and Haitian Sun and Michael Collins and William W. Cohen (2021): <a href="https://aclanthology.org/2021.deelio-1.3.pdf">Investigating the Effect of Background Knowledge on Natural Questions</a> in DeeLIO-2021.<br><ul><li><font size=-1>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</font></ul>
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen (2021): <a href="https://arxiv.org/abs/2110.06176">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</a> in ICLR 2021.<br><ul><li><font size=-1>Similar to the Entities-as-Experts model, but uses a much larger memory of entity mentions, which allows the model to potentially provide meaningful provenance for information. The model, called TOME, outperforms Entities-as-Experts on several tasks, and required some non-trivial technical innovations relating to memory pre-training and efficient retrieval.</font></ul>
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2021): <a href="https://arxiv.org/abs/2106.00200">End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents</a> in preparation.<br><ul><li><font size=-1>Adapts many of the ideas used for multihop KBQA to a new task - answering multihop questions over a large document.  Retrieval steps in this "DocHopper" system retrieve passages of a document, and the retrieved items are combined with a question neurally: i.e., rather than appending text to a question and re-encoding that discrete object, what is retrieved is a vector summary of the document, which is mixed with the previous question encoding.  This is fast, fully differentiable, allows retrieval of large document subsections, and gets a new SOTA on three datasets.</font></ul>
</li>
<li>Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan Salakhutdinov, William W. Cohen (2021): <a href="https://arxiv.org/abs/2102.07043">Reasoning Over Virtual Knowledge Bases With Open Predicate Relations</a> in ICML2021.<br><ul><li><font size=-1>Modifies the FILM model by using a virtual KB of small text passages containing pairs of entities.  This required adding a Matching-the-Blanks pretraining phase, but got strong results on a number of QA-from-corpora tasks.</font></ul>
</li>
<li>Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, William W. Cohen (2021): <a href="https://openreview.net/forum?id=yRJUNfGzdsT">Open Question Answering Over Tables and Text</a> in ICLR-2021.<br><ul><li><font size=-1>Answering open QA multi-hop questions over tables and text with a clever ``early fusion'' idea, which proposes and indexes likely reasoning chains, and uses large-document Transformers to merge these noisy evidence chains.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2021): <a href="https://aclanthology.org/2021.naacl-main.288/">Adaptable and Interpretable Neural Memory Over Symbolic Knowledge</a> in NAACL-2021.<br><ul><li><font size=-1>Most recent paper on Fact-Injected Language Model (FILM), which includes an Entities-as-Experts style memory of neural entity encodings, plus a second "fact memory" of KG triples.  FILM has good results on KBQA tasks, and allows one to use an edited KB with retraining.</font></ul>
</li>
<li>Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang Ren, William W. Cohen (2020): <a href="https://arxiv.org/abs/2010.14439">Differentiable Open-Ended Commonsense Reasoning</a> in NAACL-2021.<br><ul><li><font size=-1>Extends DrKIT's virtual KB to a corpus of documents of common-sense statements ("facts").  In DrFact, entities are replaced by noisy and ambiguous concepts, and navigation is between documents with overlapping sets of mentions.  Also introduces new "open" tasks for common-sense QA.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2020): <a href="https://arxiv.org/abs/2007.00849">Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge</a> in arxiv.<br><ul><li><font size=-1>Earlier draft of the NAACL paper on FILM (Fact-Injected LM).</font></ul>
</li>
<li>Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, William W. Cohen (2020): <a href="https://openreview.net/forum?id=SJxstlHFPH">Differentiable Reasoning over a Virtual Knowledge Base</a> in ICLR-2020.<br><ul><li><font size=-1>Describes DrKIT, which allows one to answer multihop chain queries on a "virtual KB"---a corpus of entity-linked documents.  In DrKIT, entity mentions are indexed for neural retrieval with a rich representation of their context, and reasoning consists of navigating between co-occurring mentions.</font></ul>
</li>
</ol>
<p align="center">[<a href="pubs-s.html">Selected papers</a>| By topic: <a href="pubs-G.html">GNAT System</a>|  <a href="pubs-R.html">Retrieval Augmented LMs</a>|  <a href="pubs-a.html">Applications</a>|  <a href="pubs-c.html">Collaborative Filtering</a>|  <a href="pubs-d.html">Intelligent Tutoring</a>|  <a href="pubs-e.html">Explanation-Based Learning</a>|  <a href="pubs-f.html">Formal Results</a>|  <a href="pubs-g.html">Learning in Graphs</a>|  <a href="pubs-i.html">Inductive Logic Programming</a>|  <a href="pubs-k.html">Neural Knowledge Representation</a>|  <a href="pubs-l.html">Topic Modeling</a>|  <a href="pubs-m.html">Matching/Data Integration</a>|  <a href="pubs-n.html">Deep Learning</a>|  <a href="pubs-r.html">Rule Learning</a>|  <a href="pubs-t.html">Text Categorization</a>|  <a href="pubs-x.html">Info Extraction/Reading/QA</a>|  By year: <a href="pubs.html">All papers</a>]</p>
</body></html>
