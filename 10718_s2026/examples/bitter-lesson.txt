This text seems to suggest that the only valuable contributions in ML are ones that scale, but this seems like an awfully pessimistic simplification. 
1. The paper states "in the long run it plateaus and even inhibits further progress", it being building human knowledge into AI. But there is only so much search and learning through compute can grow before hitting a barrier. Wouldn't falling back to build human knowledge be the only solution then?   2. LLMs today mostly reflect the same strategy of scaling by search and learning over knowledge acquisition. But RLHF has been one of the driving forces behind LLM advancements in the past few years, does this violate the Bitter Lesson? 
Interestingly, continuing on the chess example, Stockfish, one of the most prominent chess engines today, used to use human written evaluation functions until very recently. These evaluation functions were meant to guide the engine's move search. In 2020, they began to move away from using it, and by 2023, it was replaced altogether by a neural network based representation of evaluating various chess positions, demonstrating that this is still a relevant problem in game-playing even nowadays.
Are there current methods if any that allows for domain knowledge to be incorporated without blocking scalability and generalizability?
How can we incorporate human knowledge in a way that does not prevent methods from scaling with computation?
Is there a distinction between 'human knowledge' and 'inductive bias,' and if so, does the Bitter Lesson apply equally to both?
It is a bitter lesson that scaling beats everything else, and I do find it disappointing, but I find it difficult to come to a satisfying reason for WHY I find it disappointing. It might just be latent Luddite tendencies of mine.
The Bitter Lesson relies heavily on continued gains from scaling computation, historically justified by Moore’s Law. As Moore’s Law slows and high-quality data (e.g., the Internet) becomes a bottleneck, does this challenge the long-term validity of the Bitter Lesson? Does progress now require more human ingenuity rather than simply more scaling?
The bitter lesson argues that general methods that scale with computation inevitably outperform approaches that incorporate human knowledge. But does this still hold when success if not defined merely by performance, but also by safety, reliability, and accountability (as discussed in the first AI paper)?
In some domains, ML progress has seemingly plateaued (e.g. tree methods combined with extensive feature engineering often work best on tabular datasets). How does this relate to the bitter lesson?
The essay depends heavily on continued exponential growth in compute. What happens to AI research methodology if compute growth slowed dramatically (either due to us hitting a fundamental ceiling, or due to policy restrictions)? Does domain knowledge become useful again? The essay seems to say that we should just wait till compute improves again, but that would not make sense if the stall lasts decades, and we can see improvement (albeit temporary) by leveraging human knowledge.
What counts as “human knowledge baked in” versus “general method” in that domain?
What research directions in today’s LLM and agent ecosystems are likely to become the new “bitter lessons,” delivering short-term wins that will not scale with increasing compute, and, in the context of AGI, should we expect continued scaling of general-purpose learning and search to be sufficient on its own, or will attaining AGI inevitably require integrating substantial human knowledge or other forms of non-scaling structure?
One limitation of the Bitter Lesson is the practical mismatch for engineering teams that need to ship systems under fixed compute and time constraints. For example, autonomous vehicles have human-designed structure and heuristics that are required to make the vehicle safe and reliable. In this sense, the Bitter Lesson becomes less applicable to practical deployments and to smaller companies and labs that don’t have access to large compute resources.
How to find the "general purpose methods"?
What parts of human reasoning appear too complex to encode directly?
1. The essay argues that, over long horizons, “general methods leveraging computation” dominate because compute becomes cheaper and more available. If Moore’s law-like scaling slows or shifts to more expensive regimes, what parts of the argument remain valid, and what parts break down? 2. The essay suggests we should build in “meta-methods” that can inherently discover complexity rather than building in our own discoveries. Where should we draw the line between acceptable built-ins and human-centric knowledge that risks locking in the wrong abstractions?
In principal, the bitter lesson argues that any human-priors are a bottleneck to a system having super-human performance. Yet, in my opinion, a human prior serves as a great starting point to build off of. Eg. doing RLVR on an LLM trained on human data is better than starting from scratch.
The author's point has been proven by the ever scaling models and data used to trained those models and the diminishing human prior injected in big machine learning systems, although smaller systems might still take advantage of it. His take on the nature of the use of domain knowledge is insightful: it's to cap machine's reasoning/perceptual capability to ours, despite the endlessly complex outer world. Nowadays, human prior exists in small systems and as the intuition for the "meta-methods", such as the attention mechanism.
While I agree with the general premise, sometimes additional complexity is necessary for better ML methods. For example, I can think about convolutional networks vs fully connected networks. While fully connected networks are simpler, they are abandoned for convolutional networks for image classification.  So a better way to probably think about this is "only complexity that scales better with data should be considered". In light of this, how should researchers propose new methods/think about every bit of additional complexity they add to their new methods? What evidence would justify such complexity?
I tend to agree with most of the post. However, I don't think it's correct to dismiss the intuitions of the researchers in favor of empirical results.  Although Deep Blue's empirical results were technically brilliant, in the sense that they revealed how far simple alpha-beta search methods and hand-tuned heuristics could go, I would say complaints at the time that Deep Blue's methods were clearly not "correct" in the intuitive sense were entirely justified. In fact, it could be argued that it is the same empirical victories that tout "inhuman" and inefficient algorithms that result in a saturation of ideas due to the allure of measurable progress.
This text suggests that researchers waste time on human-knowledge approaches when they should focus on scalable computation. But doesn't this lead to enormous resource wastage?Training increasingly large models that consume massive energy and computational resources is not sustainable after a point. Is the bitter lesson contingent on Moore's Law continuing indefinitely, or do physical and environmental limits force us back toward efficiency and human knowledge?  The author says that we want "AI agents that can discover like we can, not which contain what we have discovered." But if AI systems discover things through methods completely different from human reasoning (pure statistical pattern-matching at scale), can we trust or validate the discoveries if we cannot explain them ourselves? How can we ensure these systems haven't learned bad correlations or developed solutions that work in training but fail in novel situations?  I feel like here, the bitter lesson conflicts with the need for interpretable, trustworthy AI, which we ultimately need because we are the ones using it in the end. We still need human domain experts to validate that the system is learning the right things.
1. If the most effective AI systems are those that ignore human designed structures in favour of massive search and learning, does the bitter lesson imply that AI interpretability is fighting a losing battle?  2. What changes to benchmarks, funding and publication norms would reward approaches that scale with compute rather than ones that get short term gains from hand engineered domain knowledge?
I think it's interesting that this was written in 2019 before language-based foundation models became popular. In some ways, the popularity of foundation models that can be fine-tuned for particular tasks clearly support the argument here. They are general methods, and they leverage computation, both for training and inference.   Sutton says the two methods that seem to scale arbitrarily are search and learning. Clearly, foundation models heavily rely on learning. My question is, do foundation models support the idea that search is as important as learning? Or do foundation models show that learning has passed search in importance?
Historically, researchers may struggle to keep up a theoretical understanding of compute-focused methods as they progresses quickly. For example, deep neural networks and transformers can lack interpretability and, which can make accountability and oversight difficult. Are these concerns worth slowing progress, or should they naturally occur at a much slower scale? 
