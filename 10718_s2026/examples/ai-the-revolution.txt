What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?
Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?
The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g
With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?
What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?
How would the author would feel differently today about the progress in human-imitative AI? Would the progress in robotics be significant to change the author's opinion or are we still limited by the slow rise in accountability, interpretability and fairness? 
If intelligence is not the main problem, what should we be focusing on instead?
1. In practice, how should human values/input be integrated into ML system design and evaluation? Should they be integrated as product requirements, as governance, as objective terms that must be optimized, or as audits post-deployment? 2. The essay claims we are missing an “engineering discipline” with principles of analysis and design for the socio-technical systems it describes. What would be the core abstractions of this proposed discipline (e.g.: reliability, robustness, incentive alignment, etc.)? How would we validate empirically that these ideas generalize across domains?
It is an interesting question to ask: what is the optimal, practical problem which ML can solve which would benefit society the most? Would current work on Language Models be in that list?
Jordan strongly advocates IA (Intelligent Enhancement) rather than AI (Artificial Intelligence). However, in the real business logic, "enhancing humans" is often just a transitional stage of "replacing humans" (for instance, Code agent might eventually write all the code). Is Jordan's so-called IA and II (intelligent infrastructure) merely an illusory concept and IA will necessarily slide towards AI?
The author argues that what we commonly call "AI" today is not the emergence of human-like intelligence or a step toward AGI, but the development of large-scale inference and decision-making systems built from data, algorithms, and infrastructure. His concern is whether these systems can be engineered to be a reliable, context-aware, and accountable when they operate at societal scale. In contrast, contemporary discussions often describe AI agents as autonomous, goal-directed systems and often frame them as incremental steps toward AGI. This creates a tension with the author's implicit understanding of AGI as a human-imitative form of general intelligence rather than a system-level accumulation of capabilities. In this context, is pursuing AGI still a meaningful goal? If so, what purpose would AGI serve beyond increasingly powerful systems? Or more specifically, how would AGI behave differently from, or meaningfully outperform, large-scale agentic and infrastructural systems?
The article emphasizes that II systems must operate on rapidly changing, globally incoherent knowledge. What major research problems do you see at the intersection of distributed systems, databases, and ML to make that feasible at a large scale?
This article is from 2019, and obviously predates LLMs. Do we think LLMs have helped or have the potential to solve IA or II system problems? 
Since the article was written over 4 years ago, has the recent advancements in large reasoning models challenged or reinforced the authors claim that IA and II remain as critical, independent problems to solve?
1. Is human level AI actually a low bar? If an AI makes fewer mistakes than a human but makes weirder mistakes (like the ultrasound machine seeing white noise as a disease) do we forgive it?   2. Why are we terrified of one robot crash but ignore thousands of human crashes?
Could there be a "bitter lesson" for II problems- that attempts to hand-design/encode fair, safe, human centric systems will eventually be outperformed by systems that simply learn from massive amounts of data about human behaviour and preferences (say, if we manage to reduce bias and other unwanted traits of the data)?
how should we incorporate the perspectives of social sciences and humanities into the development of AI?
Now that human-imitating AI has possessed intelligence on par with average human, and knowledge perhaps greater than all of us, can the author's concern about IA/II be addressed with modern systems like agentic AI? For example, could it resolve the issue of his wife's false positive diagnosis, where he believed contemporary AI wasn't the solution?
Given the premise that we are currently building 'societal-scale inference systems' without any technical rigour of an established engineering discipline, how can we design these inference systems to know when they are 'untrustable' because the data they were trained on doesn't match the current situation? I think this is just as hard as knowing when to re-do an old study that used different machines for measurement, and, in a sense, also a problem with established disciplines.
Since this article was written, the developments in generative AI have led to many advancements for human-imitative AI. Companies and researchers are applying these to other associated advancements in the II and IA areas. Would generative AI be able to overcome the difficulties mentioned in the article about solving II / IA problems without necessarily having dedicated problem solving in their own domains?
How can we facilitate AI being treated as an engineering discipline when money is being poured into superintelligence ideas?
What risks grow when leaders frame systems as intelligent rather than engineered?

