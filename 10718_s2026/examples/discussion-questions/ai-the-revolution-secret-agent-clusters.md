Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', "What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions are concerned with establishing rigorous standards and metrics for evaluating AI systems - one focusing on validation requirements before deployment, the other on distinguishing genuine scientific advances from incremental engineering improvements and marketing hype.
0, 3 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', "How would the author would feel differently today about the progress in human-imitative AI? Would the progress in robotics be significant to change the author's opinion or are we still limited by the slow rise in accountability, interpretability and fairness? \n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions are concerned with ensuring AI systems meet standards for accountability, interpretability, and fairness - q1 asks what must be validated before deployment, while s2 questions whether recent progress has adequately addressed these concerns.
0, 4 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'If intelligence is not the main problem, what should we be focusing on instead?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', '1. In practice, how should human values/input be integrated into ML system design and evaluation? Should they be integrated as product requirements, as governance, as objective terms that must be optimized, or as audits post-deployment? 2. The essay claims we are missing an “engineering discipline” with principles of analysis and design for the socio-technical systems it describes. What would be the core abstractions of this proposed discipline (e.g.: reliability, robustness, incentive alignment, etc.)? How would we validate empirically that these ideas generalize across domains?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address the need for systematic validation, specification, and design principles for AI systems before deployment, with particular emphasis on incentive alignment and ensuring that human values and concerns are properly integrated into the system design and evaluation process.
0, 6 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'It is an interesting question to ask: what is the optimal, practical problem which ML can solve which would benefit society the most? Would current work on Language Models be in that list?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'Jordan strongly advocates IA (Intelligent Enhancement) rather than AI (Artificial Intelligence). However, in the real business logic, "enhancing humans" is often just a transitional stage of "replacing humans" (for instance, Code agent might eventually write all the code). Is Jordan\'s so-called IA and II (intelligent infrastructure) merely an illusory concept and IA will necessarily slide towards AI?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'The author argues that what we commonly call "AI" today is not the emergence of human-like intelligence or a step toward AGI, but the development of large-scale inference and decision-making systems built from data, algorithms, and infrastructure. His concern is whether these systems can be engineered to be a reliable, context-aware, and accountable when they operate at societal scale. In contrast, contemporary discussions often describe AI agents as autonomous, goal-directed systems and often frame them as incremental steps toward AGI. This creates a tension with the author\'s implicit understanding of AGI as a human-imitative form of general intelligence rather than a system-level accumulation of capabilities. In this context, is pursuing AGI still a meaningful goal? If so, what purpose would AGI serve beyond increasingly powerful systems? Or more specifically, how would AGI behave differently from, or meaningfully outperform, large-scale agentic and infrastructural systems?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address the engineering and deployment of AI systems, specifically concerning the need for proper specification, validation, accountability, and consideration of failure modes and alignment before systems operate at scale.
0, 9 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'The article emphasizes that II systems must operate on rapidly changing, globally incoherent knowledge. What major research problems do you see at the intersection of distributed systems, databases, and ML to make that feasible at a large scale?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'This article is from 2019, and obviously predates LLMs. Do we think LLMs have helped or have the potential to solve IA or II system problems? \n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'Since the article was written over 4 years ago, has the recent advancements in large reasoning models challenged or reinforced the authors claim that IA and II remain as critical, independent problems to solve?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address the critical need to ensure AI systems are safe, trustworthy, and properly validated - one focusing on pre-deployment specifications and validation checks, the other on whether interpretability and intelligence augmentation remain essential safeguards as AI advances.
0, 12 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', '1. Is human level AI actually a low bar? If an AI makes fewer mistakes than a human but makes weirder mistakes (like the ultrasound machine seeing white noise as a disease) do we forgive it?   2. Why are we terrified of one robot crash but ignore thousands of human crashes?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'Could there be a "bitter lesson" for II problems- that attempts to hand-design/encode fair, safe, human centric systems will eventually be outperformed by systems that simply learn from massive amounts of data about human behaviour and preferences (say, if we manage to reduce bias and other unwanted traits of the data)?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address the challenge of ensuring AI systems are safe, fair, and properly aligned with human values - one focusing on what must be validated before deployment, and the other questioning whether hand-designed safety and fairness measures will be effective.
0, 14 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'how should we incorporate the perspectives of social sciences and humanities into the development of AI?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', "Now that human-imitating AI has possessed intelligence on par with average human, and knowledge perhaps greater than all of us, can the author's concern about IA/II be addressed with modern systems like agentic AI? For example, could it resolve the issue of his wife's false positive diagnosis, where he believed contemporary AI wasn't the solution?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', "Given the premise that we are currently building 'societal-scale inference systems' without any technical rigour of an established engineering discipline, how can we design these inference systems to know when they are 'untrustable' because the data they were trained on doesn't match the current situation? I think this is just as hard as knowing when to re-do an old study that used different machines for measurement, and, in a sense, also a problem with established disciplines.\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address the need for rigorous validation and checking mechanisms to ensure that large-scale AI systems are trustworthy and can detect when they are operating outside their valid domain, whether through shift detection, provenance checks, or identifying data-situation mismatches.
0, 17 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'Since this article was written, the developments in generative AI have led to many advancements for human-imitative AI. Companies and researchers are applying these to other associated advancements in the II and IA areas. Would generative AI be able to overcome the difficulties mentioned in the article about solving II / IA problems without necessarily having dedicated problem solving in their own domains?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'How can we facilitate AI being treated as an engineering discipline when money is being poured into superintelligence ideas?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions express concern about the need for more rigorous engineering practices and validation in AI development, rather than pursuing speculative approaches like superintelligence without proper safety specifications and discipline.
0, 19 are similar
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', 'What risks grow when leaders frame systems as intelligent rather than engineered?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('What must be specified/validated before deployment (provenance, shift checks, utility functions, incentive alignment, failure modes)?\n', '\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', "What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', '1. In practice, how should human values/input be integrated into ML system design and evaluation? Should they be integrated as product requirements, as governance, as objective terms that must be optimized, or as audits post-deployment? 2. The essay claims we are missing an “engineering discipline” with principles of analysis and design for the socio-technical systems it describes. What would be the core abstractions of this proposed discipline (e.g.: reliability, robustness, incentive alignment, etc.)? How would we validate empirically that these ideas generalize across domains?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'Jordan strongly advocates IA (Intelligent Enhancement) rather than AI (Artificial Intelligence). However, in the real business logic, "enhancing humans" is often just a transitional stage of "replacing humans" (for instance, Code agent might eventually write all the code). Is Jordan\'s so-called IA and II (intelligent infrastructure) merely an illusory concept and IA will necessarily slide towards AI?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions explore the tension between AI augmenting versus replacing human decision-making, with q1 questioning why AI adoption remains limited in medical practice despite potential, and s2 questioning whether IA (intelligent augmentation) is sustainable or inevitably slides toward AI replacement.
1, 7 are similar
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'The author argues that what we commonly call "AI" today is not the emergence of human-like intelligence or a step toward AGI, but the development of large-scale inference and decision-making systems built from data, algorithms, and infrastructure. His concern is whether these systems can be engineered to be a reliable, context-aware, and accountable when they operate at societal scale. In contrast, contemporary discussions often describe AI agents as autonomous, goal-directed systems and often frame them as incremental steps toward AGI. This creates a tension with the author\'s implicit understanding of AGI as a human-imitative form of general intelligence rather than a system-level accumulation of capabilities. In this context, is pursuing AGI still a meaningful goal? If so, what purpose would AGI serve beyond increasingly powerful systems? Or more specifically, how would AGI behave differently from, or meaningfully outperform, large-scale agentic and infrastructural systems?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions express concern about the gap between AI's potential capabilities and its actual practical application and purpose, questioning what meaningful role advanced AI systems should serve in real-world contexts.
1, 8 are similar
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'This article is from 2019, and obviously predates LLMs. Do we think LLMs have helped or have the potential to solve IA or II system problems? \n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'Since the article was written over 4 years ago, has the recent advancements in large reasoning models challenged or reinforced the authors claim that IA and II remain as critical, independent problems to solve?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions consider how recent advances in AI technology (medical AI applications and large reasoning models) might have changed or should change the relevance of the paper's original claims and concerns.
1, 11 are similar
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'Could there be a "bitter lesson" for II problems- that attempts to hand-design/encode fair, safe, human centric systems will eventually be outperformed by systems that simply learn from massive amounts of data about human behaviour and preferences (say, if we manage to reduce bias and other unwanted traits of the data)?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', "Now that human-imitating AI has possessed intelligence on par with average human, and knowledge perhaps greater than all of us, can the author's concern about IA/II be addressed with modern systems like agentic AI? For example, could it resolve the issue of his wife's false positive diagnosis, where he believed contemporary AI wasn't the solution?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions address whether AI systems can effectively solve medical diagnosis and decision-making problems, and whether the potential of AI to address these medical challenges has been or can be realized in practice.
1, 15 are similar
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', "Given the premise that we are currently building 'societal-scale inference systems' without any technical rigour of an established engineering discipline, how can we design these inference systems to know when they are 'untrustable' because the data they were trained on doesn't match the current situation? I think this is just as hard as knowing when to re-do an old study that used different machines for measurement, and, in a sense, also a problem with established disciplines.\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', 'How can we facilitate AI being treated as an engineering discipline when money is being poured into superintelligence ideas?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('Many medical errors stem from human limitations such as fatigue and imperfect judgment. What role can AI play in reducing these errors in tasks like diagnosis, treatment decisions, and risk evaluation? If the potential is significant, why do we still see relatively limited use of AI in everyday medical practice?\n', '\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', "What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions examine the tension between data-driven empirical approaches and theoretical/explanatory principles in AI, questioning whether current machine learning advances represent genuine scientific progress or merely incremental engineering advances lacking in theoretical grounding.
2, 1 are similar
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'It is an interesting question to ask: what is the optimal, practical problem which ML can solve which would benefit society the most? Would current work on Language Models be in that list?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'Since the article was written over 4 years ago, has the recent advancements in large reasoning models challenged or reinforced the authors claim that IA and II remain as critical, independent problems to solve?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions examine whether recent technological advancements and the passage of time since the article was written have validated, challenged, or altered the relevance of the author's original arguments and criticisms.
2, 10 are similar
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'how should we incorporate the perspectives of social sciences and humanities into the development of AI?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'Since this article was written, the developments in generative AI have led to many advancements for human-imitative AI. Companies and researchers are applying these to other associated advancements in the II and IA areas. Would generative AI be able to overcome the difficulties mentioned in the article about solving II / IA problems without necessarily having dedicated problem solving in their own domains?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions ask whether recent advances in AI (data-driven approaches and generative AI) can overcome or address the fundamental problems and limitations that the article discusses regarding interpretability, intelligence augmentation, and human-imitative AI.
2, 16 are similar
Calling strong_common_concerns ('The author criticizes the current approach of "gathering data, deploying deep learning infrastructure, and demonstrating systems that mimic narrowly-defined human skills with little in the way of emerging explanatory principles." However, engineering and science has often historically progressed through empirical trial-and-error before theoretical principles emerged. Are we in an early phase where data-driven approaches will eventually lead to the explanatory principles the author talks about? Is this fundamentally the wrong path and should our focus really be only on the theory? https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g\n', 'What risks grow when leaders frame systems as intelligent rather than engineered?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n', "What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n', 'If intelligence is not the main problem, what should we be focusing on instead?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ('With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n', 'Jordan strongly advocates IA (Intelligent Enhancement) rather than AI (Artificial Intelligence). However, in the real business logic, "enhancing humans" is often just a transitional stage of "replacing humans" (for instance, Code agent might eventually write all the code). Is Jordan\'s so-called IA and II (intelligent infrastructure) merely an illusory concept and IA will necessarily slide towards AI?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions examine the tension between AI systems augmenting versus replacing human decision-making, and whether augmentation is a sustainable goal or merely a transitional stage toward full replacement.
3, 5 are similar
Calling strong_common_concerns ('With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n', "Now that human-imitating AI has possessed intelligence on par with average human, and knowledge perhaps greater than all of us, can the author's concern about IA/II be addressed with modern systems like agentic AI? For example, could it resolve the issue of his wife's false positive diagnosis, where he believed contemporary AI wasn't the solution?\n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions explore whether AI systems can effectively augment human decision-making and address the author's concerns about AI's role in critical decisions, particularly in medical diagnosis and treatment.
3, 13 are similar
Calling strong_common_concerns ('With reference to “Intelligence Augmentation” described in the article, what should the true goal of AI systems be: replacing human decision-making or augmenting it?\n', '\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned no common concerns
Calling strong_common_concerns ("What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n", "How would the author would feel differently today about the progress in human-imitative AI? Would the progress in robotics be significant to change the author's opinion or are we still limited by the slow rise in accountability, interpretability and fairness? \n")...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions critically examine whether recent AI progress represents genuine breakthroughs or overstated advances, and both acknowledge the paper's limitations in addressing modern AI developments and the need to reassess claims about AI's true capabilities.
4, 0 are similar
Calling strong_common_concerns ("What the public and much of industry calls “AI” today is largely machine learning (ML), not genuine artificial intelligence with human-like cognition. The real advance has been data-driven statistical methods and systems engineering, not a revolution in autonomous reasoning or general intelligence. There’s overhyping of AI’s capabilities in media and public discourse, creating confusion between marketing success and scientific revolution.  In light of all of these (given the limitation of this paper which doesn't discuss modern reasoning systems), what empirical metrics can distinguish between incremental engineering advances and genuine scientific revolutions in AI?\n", '1. In practice, how should human values/input be integrated into ML system design and evaluation? Should they be integrated as product requirements, as governance, as objective terms that must be optimized, or as audits post-deployment? 2. The essay claims we are missing an “engineering discipline” with principles of analysis and design for the socio-technical systems it describes. What would be the core abstractions of this proposed discipline (e.g.: reliability, robustness, incentive alignment, etc.)? How would we validate empirically that these ideas generalize across domains?\n')...
calling service=anthropic model=claude-haiku-4-5-20251001
...strong_common_concerns returned Strong commonality: Both questions emphasize the need for rigorous, empirical frameworks and metrics to evaluate AI/ML systems, rather than accepting marketing hype or ad-hoc approaches. q1 seeks empirical metrics to distinguish genuine scientific advances from incremental engineering, while s2 calls for core abstractions and empirical validation of an engineering discipline for socio-technical systems.
4, 2 are similar
============================================================
0 What must be specified/validated before deployment (provenance, shift
checks, utility functions, incentive alignment, failure modes)?

3 With reference to “Intelligence Augmentation” described in the
article, what should the true goal of AI systems be: replacing human
decision-making or augmenting it?

4 What the public and much of industry calls “AI” today is largely
machine learning (ML), not genuine artificial intelligence with human-
like cognition. The real advance has been data-driven statistical
methods and systems engineering, not a revolution in autonomous
reasoning or general intelligence. There’s overhyping of AI’s
capabilities in media and public discourse, creating confusion between
marketing success and scientific revolution.  In light of all of these
(given the limitation of this paper which doesn't discuss modern
reasoning systems), what empirical metrics can distinguish between
incremental engineering advances and genuine scientific revolutions in
AI?

6 If intelligence is not the main problem, what should we be focusing on
instead?

9 Jordan strongly advocates IA (Intelligent Enhancement) rather than AI
(Artificial Intelligence). However, in the real business logic,
"enhancing humans" is often just a transitional stage of "replacing
humans" (for instance, Code agent might eventually write all the
code). Is Jordan's so-called IA and II (intelligent infrastructure)
merely an illusory concept and IA will necessarily slide towards AI?

12 This article is from 2019, and obviously predates LLMs. Do we think
LLMs have helped or have the potential to solve IA or II system
problems?

14 1. Is human level AI actually a low bar? If an AI makes fewer mistakes
than a human but makes weirder mistakes (like the ultrasound machine
seeing white noise as a disease) do we forgive it?   2. Why are we
terrified of one robot crash but ignore thousands of human crashes?

17 Now that human-imitating AI has possessed intelligence on par with
average human, and knowledge perhaps greater than all of us, can the
author's concern about IA/II be addressed with modern systems like
agentic AI? For example, could it resolve the issue of his wife's
false positive diagnosis, where he believed contemporary AI wasn't the
solution?

19 Since this article was written, the developments in generative AI have
led to many advancements for human-imitative AI. Companies and
researchers are applying these to other associated advancements in the
II and IA areas. Would generative AI be able to overcome the
difficulties mentioned in the article about solving II / IA problems
without necessarily having dedicated problem solving in their own
domains?
============================================================
1 Many medical errors stem from human limitations such as fatigue and
imperfect judgment. What role can AI play in reducing these errors in
tasks like diagnosis, treatment decisions, and risk evaluation? If the
potential is significant, why do we still see relatively limited use
of AI in everyday medical practice?

7 1. In practice, how should human values/input be integrated into ML
system design and evaluation? Should they be integrated as product
requirements, as governance, as objective terms that must be
optimized, or as audits post-deployment? 2. The essay claims we are
missing an “engineering discipline” with principles of analysis and
design for the socio-technical systems it describes. What would be the
core abstractions of this proposed discipline (e.g.: reliability,
robustness, incentive alignment, etc.)? How would we validate
empirically that these ideas generalize across domains?

8 It is an interesting question to ask: what is the optimal, practical
problem which ML can solve which would benefit society the most? Would
current work on Language Models be in that list?

11 The article emphasizes that II systems must operate on rapidly
changing, globally incoherent knowledge. What major research problems
do you see at the intersection of distributed systems, databases, and
ML to make that feasible at a large scale?

15 Could there be a "bitter lesson" for II problems- that attempts to
hand-design/encode fair, safe, human centric systems will eventually
be outperformed by systems that simply learn from massive amounts of
data about human behaviour and preferences (say, if we manage to
reduce bias and other unwanted traits of the data)?
============================================================
2 The author criticizes the current approach of "gathering data,
deploying deep learning infrastructure, and demonstrating systems that
mimic narrowly-defined human skills with little in the way of emerging
explanatory principles." However, engineering and science has often
historically progressed through empirical trial-and-error before
theoretical principles emerged. Are we in an early phase where data-
driven approaches will eventually lead to the explanatory principles
the author talks about? Is this fundamentally the wrong path and
should our focus really be only on the theory?
https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g

1 Many medical errors stem from human limitations such as fatigue and
imperfect judgment. What role can AI play in reducing these errors in
tasks like diagnosis, treatment decisions, and risk evaluation? If the
potential is significant, why do we still see relatively limited use
of AI in everyday medical practice?

10 The author argues that what we commonly call "AI" today is not the
emergence of human-like intelligence or a step toward AGI, but the
development of large-scale inference and decision-making systems built
from data, algorithms, and infrastructure. His concern is whether
these systems can be engineered to be a reliable, context-aware, and
accountable when they operate at societal scale. In contrast,
contemporary discussions often describe AI agents as autonomous, goal-
directed systems and often frame them as incremental steps toward AGI.
This creates a tension with the author's implicit understanding of AGI
as a human-imitative form of general intelligence rather than a
system-level accumulation of capabilities. In this context, is
pursuing AGI still a meaningful goal? If so, what purpose would AGI
serve beyond increasingly powerful systems? Or more specifically, how
would AGI behave differently from, or meaningfully outperform, large-
scale agentic and infrastructural systems?

16 how should we incorporate the perspectives of social sciences and
humanities into the development of AI?
============================================================
3 With reference to “Intelligence Augmentation” described in the
article, what should the true goal of AI systems be: replacing human
decision-making or augmenting it?

5 How would the author would feel differently today about the progress
in human-imitative AI? Would the progress in robotics be significant
to change the author's opinion or are we still limited by the slow
rise in accountability, interpretability and fairness?

13 Since the article was written over 4 years ago, has the recent
advancements in large reasoning models challenged or reinforced the
authors claim that IA and II remain as critical, independent problems
to solve?
============================================================
4 What the public and much of industry calls “AI” today is largely
machine learning (ML), not genuine artificial intelligence with human-
like cognition. The real advance has been data-driven statistical
methods and systems engineering, not a revolution in autonomous
reasoning or general intelligence. There’s overhyping of AI’s
capabilities in media and public discourse, creating confusion between
marketing success and scientific revolution.  In light of all of these
(given the limitation of this paper which doesn't discuss modern
reasoning systems), what empirical metrics can distinguish between
incremental engineering advances and genuine scientific revolutions in
AI?

0 What must be specified/validated before deployment (provenance, shift
checks, utility functions, incentive alignment, failure modes)?

2 The author criticizes the current approach of "gathering data,
deploying deep learning infrastructure, and demonstrating systems that
mimic narrowly-defined human skills with little in the way of emerging
explanatory principles." However, engineering and science has often
historically progressed through empirical trial-and-error before
theoretical principles emerged. Are we in an early phase where data-
driven approaches will eventually lead to the explanatory principles
the author talks about? Is this fundamentally the wrong path and
should our focus really be only on the theory?
https://hdsr.mitpress.mit.edu/pub/wot7mkc1#t9e87b477g
