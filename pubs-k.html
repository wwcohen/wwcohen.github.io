<html><head><title>William W. Cohen's Papers: Neural Knowledge Representation</title>
<link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body><h3>William W. Cohen's Papers: Neural Knowledge Representation</h3>
<ol>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2021): <a href="https://arxiv.org/abs/2110.06884">ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers</a> in ACL 2022.<br><ul><li><font size=-1>A novel dataset with (1) long context documents containing information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning. Intended as a more realistic version of ShARC, a QA task considered in 'End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents'</font></ul>
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen (2021): <a href="https://arxiv.org/abs/2110.06176">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</a> in ICLR 2021.<br><ul><li><font size=-1>Similar to the Entities-as-Experts model, but uses a much larger memory of entity mentions, which allows the model to potentially provide meaningful provenance for information. The model, called TOME, outperforms Entities-as-Experts on several tasks, and required some non-trivial technical innovations relating to memory pre-training and efficient retrieval.</font></ul>
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2021): <a href="https://arxiv.org/abs/2106.00200">End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents</a> in preparation.<br><ul><li><font size=-1>Adapts many of the ideas used for multihop KBQA to a new task - answering multihop questions over a large document.  Retrieval steps in this "DocHopper" system retrieve passages of a document, and the retrieved items are combined with a question neurally: i.e., rather than appending text to a question and re-encoding that discrete object, what is retrieved is a vector summary of the document, which is mixed with the previous question encoding.  This is fast, fully differentiable, allows retrieval of large document subsections, and gets a new SOTA on three datasets.</font></ul>
</li>
<li>Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, William W. Cohen (2021): <a href="https://openreview.net/forum?id=yRJUNfGzdsT">Open Question Answering Over Tables and Text</a> in ICLR2021.<br><ul><li><font size=-1>Answering open QA multi-hop questions over tables and text with a clever ``early fusion'' idea, which proposes and indexes likely reasoning chains, and uses large-document Transformers to merge these noisy evidence chains.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2020): <a href="https://aclanthology.org/2021.naacl-main.288/">Adaptable and Interpretable Neural Memory Over Symbolic Knowledge</a> in NAACL-2021.<br><ul><li><font size=-1>Most recent paper on Fact-Injected Language Model (FILM), which includes an Entities-as-Experts style memory of neural entity encodings, plus a second "fact memory" of KG triples.  FILM has good results on KBQA tasks, and allows one to use an edited KB with retraining.</font></ul>
</li>
<li>Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang Ren, William W. Cohen (2020): <a href="https://arxiv.org/abs/2010.14439">Differentiable Open-Ended Commonsense Reasoning</a> in NAACL-2021.<br><ul><li><font size=-1>Extends DrKIT's virtual KB to a corpus of documents of common-sense statements ("facts").  In DrFact, entities are replaced by noisy and ambiguous concepts, and navigation is between documents with overlapping sets of mentions.  Also introduces new "open" tasks for common-sense QA.</font></ul>
</li>
<li>Haitian Sun, Andrew O. Arnold, Tania Bedrax-Weiss, Fernando Pereira, William W. Cohen (2020): <a href="https://arxiv.org/abs/2004.03658">Faithful Embeddings for Knowledge Base Queries</a> in NeurIPS2020.<br><ul><li><font size=-1>An extension to Neural Query Language (NQL) which extends the query language to work with a "centroid-sketch" representation of sets.  The centroid encoders a geometric area, and the sketch is a randomized data structure that adds capacity to the sketch, allowing faithful differential logical reasoning to be combined with good generalization.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2020): <a href="https://arxiv.org/abs/2007.00849">Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge</a> in arxiv.<br><ul><li><font size=-1>Earlier draft of the NAACL paper on FILM (Fact-Injected LM).</font></ul>
</li>
<li>William W. Cohen, Fan Yang, and Kathryn Rivard Mazaitis (2020): <a href="https://jair.org/index.php/jair/article/view/11944/26561">TensorLog: A Probabilistic Database Implemented Using Deep-Learning Infrastructure</a> in JAIR.<br><ul><li><font size=-1>Most complete paper on TensorLog, a predecessor of NQL/EmQL that was a Prolog-like logic, not a dataflow query language.</font></ul>
</li>
<li>William W. Cohen, Haitian Sun, R. Alex Hofer, Matthew Siegler (2020): <a href="https://arxiv.org/abs/2002.06115">Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base </a> in ICLR-2020.<br><ul><li><font size=-1>Paper on Neural Query Language (NQL) a differentiable dataflow query language.  NQL is useful for building KBQA systems that can be trained from denotations, but relies heavily on sparse-matrix operations that are not implemented in all accelerators.</font></ul>
</li>
<li>Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, William W. Cohen (2020): <a href="https://openreview.net/forum?id=SJxstlHFPH">Differentiable Reasoning over a Virtual Knowledge Base</a> in ICLR-2020.<br><ul><li><font size=-1>Describes DrKIT, which allows one to answer multihop chain queries on a "virtual KB"---a corpus of entity-linked documents.  In DrKIT, entity mentions are indexed for neural retrieval with a rich representation of their context, and reasoning consists of navigating between co-occurring mentions.</font></ul>
</li>
<li>William W. Cohen, Haitian Sun, Alex Hofer, Matthew Siegler  (2019): <a href="http://arxiv.org/abs/1905.10417">Differentiable Representations For Multihop Inference Rules</a> in arxiv.<br><ul><li><font size=-1>Earlier version of ICLR paper on NQL.</font></ul>
</li>
<li>William W. Cohen, Matthew Siegler, Alex Hofer (2019): <a href="http://arxiv.org/abs/1905.06209">Neural Query Language: A Knowledge Base Query Language for Tensorflow</a> in arxiv.<br><ul><li><font size=-1>Earlier version of ICLR paper on NQL focusing on the language constructs used.</font></ul>
</li>
</ol>
<p align="center">[<a href="pubs-s.html">Selected papers</a>| By topic: <a href="pubs-G.html">GNAT System</a>|  <a href="pubs-a.html">Applications</a>|  <a href="pubs-c.html">Collaborative Filtering</a>|  <a href="pubs-d.html">Intelligent Tutoring</a>|  <a href="pubs-e.html">Explanation-Based Learning</a>|  <a href="pubs-f.html">Formal Results</a>|  <a href="pubs-g.html">Learning in Graphs</a>|  <a href="pubs-i.html">Inductive Logic Programming</a>|  <a href="pubs-k.html">Neural Knowledge Representation</a>|  <a href="pubs-l.html">Topic Modeling</a>|  <a href="pubs-m.html">Matching/Data Integration</a>|  <a href="pubs-n.html">Deep Learning</a>|  <a href="pubs-r.html">Rule Learning</a>|  <a href="pubs-t.html">Text Categorization</a>|  <a href="pubs-x.html">Info Extraction/Reading</a>|  By year: <a href="pubs.html">All papers</a>]</p>
</body></html>
