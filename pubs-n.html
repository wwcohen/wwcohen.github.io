<html><head><title>William W. Cohen's Papers: Deep Learning</title>
<link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body><h3>William W. Cohen's Papers: Deep Learning</h3>
<ol>
<li>Yury Zemlyanskiy, Michiel de Jong, Luke Vilnis, Santiago Ontañón, William W. Cohen, Sumit Sanghai, Joshua Ainslie (2023): <a href="https://arxiv.org/abs/2308.14903">MEMORY-VQ: Compression for Tractable Internet-Scale Memory</a> in progress.
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2023): <a href="https://arxiv.org/abs/2308.08661">Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions</a> in progress.<br><ul><li><font size=-1>Following up the 'QA is the new KR' paper, we present a new collection of question-answer pairs automatically generated from Wikipedia which are more specific and ambiiguous than generated questions used in prior work, and show that this can be used to answer ambiguous questions.  On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 10-15%.  The new queston DB can also be used to improve diverse passage retrieval.</font></ul>
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Sumit Sanghai, William W. Cohen, Joshua Ainslie (2023): <a href="https://arxiv.org/abs/2306.10231">GLIMMER: generalized late-interaction memory reranker</a> in progress.
</li>
<li>Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W. Cohen (2023): <a href="https://arxiv.org/abs/2304.00186">Subject-driven Text-to-Image Generation via Apprenticeship Learning</a> in NeurIPS 2023.
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Joshua Ainslie, Sumit Sanghai, Fei Sha, William W. Cohen (2023): <a href="https://arxiv.org/abs/2301.10448">Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute</a> in ICML-2023.
</li>
<li>Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, William W. Cohen (2023): <a href="https://arxiv.org/abs/2210.02928">MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text</a> in EACL-2023.
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2022): <a href="https://arxiv.org/abs/2205.12898">Reasoning over Logically Interacted Conditions for Question Answering</a> in progress.
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William Cohen (2023): <a href="https://arxiv.org/abs/2212.08153">FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference</a> in ACL-2023 (Findings).
</li>
<li>Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen (2023): <a href="https://arxiv.org/abs/2209.14491">Re-Imagen: Retrieval-Augmented Text-to-Image Generator</a> in ICLR-2023.
</li>
<li>Julian Martin Eisenschlos, Jeremy R. Cole, Fangyu Liu, William W. Cohen (2023): <a href="https://arxiv.org/abs/2209.12153">WinoDict: Probing language models for in-context word acquisition</a> in EACL-2023.<br><ul><li><font size=-1>One of two winners of an Outstanding Paper Award at EACL.</font></ul>
</li>
<li>John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor Berg-Kirkpatrick (2023): <a href="https://arxiv.org/abs/2212.10726">Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval</a> in ACL-2023.
</li>
<li>Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen (2022): <a href="https://arxiv.org/abs/2211.12588">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</a> in progress.
</li>
<li>Wenhu Chen, William W. Cohen, Michiel De Jong, Nitish Gupta, Alessandro Presta, Pat Verga, John Wieting (2023): <a href="https://arxiv.org/abs/2207.00630">QA Is the New KR: Question-Answer Pairs as Knowledge Bases</a> in AAAI-2023.<br><ul><li><font size=-1>Proposes that symbolic KBs can be replaced with a collection of question-answer pairs automatically generated from a corpus, augmented with entity-linking annotations.  Like a symbolic KB, this representation is well-suited to structured queries involving joins and aggregation, and can support 'multi-hop' reasoning.  However, it has the advantage that the information in it is closely aligned to likely user information needs, as modeled by the question generation process.</font></ul>
</li>
<li>Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen, Yulia Tsvetkov (2022): <a href="https://arxiv.org/abs/2210.12378">Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</a> in EMNLP-2022.
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2023): <a href="https://openreview.net/forum?id=tPrRs6YB2P">Scenario-based Question Answering with Interacting Contextual Properties</a> in ICLR-2023.
</li>
<li>Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster (2022): <a href="https://arxiv.org/abs/2212.08037">Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models</a> in progress.
</li>
<li>Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, William W. Cohen (2022): <a href="https://arxiv.org/abs/2204.04581">Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering</a> in EACL-2022.<br><ul><li><font size=-1>Extends the techniques of Mention Memory in several important ways. (1) The memory is a memory of generated question-answer pairs, which is more interpretable than neural entity-mention encodings; (2) it is based on pre-trained T5, not a custom Transformer; and (3) it allows use of the token-level encoding of retrieved QA pairs as well as neural encodings of them for reasoning.  Using QA pairs instead of passages allows a clever pre-training trick for learning to retrieve, and the model greatly outperfoms a prior similar model (i.e., RePAQ) on smaller QA benchmarks.</font></ul>
</li>
<li>Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen and Donald Metzler (2022): <a href="https://arxiv.org/abs/2202.06991">Transformer Memory as a Differentiable Search Index</a> in NeurIPS 2022.
</li>
<li>Siddhant Arora, Danish Pruthi, Norman Sadeh, William W. Cohen, Zachary C. Lipton, Graham Neubig (2022): <a href="https://arxiv.org/abs/2112.09669">Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations</a> in AAAI 2022.
</li>
<li>Vidhisha Balachandran and Bhuwan Dhingra and Haitian Sun and Michael Collins and William W. Cohen (2021): <a href="https://aclanthology.org/2021.deelio-1.3.pdf">Investigating the Effect of Background Knowledge on Natural Questions</a> in DeeLIO-2021.<br><ul><li><font size=-1>Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</font></ul>
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2021): <a href="https://arxiv.org/abs/2110.06884">ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers</a> in ACL 2022.<br><ul><li><font size=-1>A novel dataset with (1) long context documents containing information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning. Intended as a more realistic version of ShARC, a QA task considered in 'End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents'</font></ul>
</li>
<li>Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, William Cohen (2021): <a href="https://arxiv.org/abs/2110.06176">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention</a> in ICLR 2021.<br><ul><li><font size=-1>Similar to the Entities-as-Experts model, but uses a much larger memory of entity mentions, which allows the model to potentially provide meaningful provenance for information. The model, called TOME, outperforms Entities-as-Experts on several tasks, and required some non-trivial technical innovations relating to memory pre-training and efficient retrieval.</font></ul>
</li>
<li>Keshav Kolluru, Martin Rezk, Pat Verga, William W. Cohen, Partha Talukdar (2021): <a href="https://arxiv.org/abs/2109.14364">Multilingual Fact Linking</a> in AKBC-2021.
</li>
<li>Julian Martin Eisenschlos, Maharshi Gor, Thomas Muller, William W. Cohen (2021): <a href="https://arxiv.org/abs/2109.04312">MATE: Multi-view Attention for Table Transformer Efficiency</a> in EMNLP-2021.
</li>
<li>Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, William W. Cohen (2021): <a href="https://arxiv.org/abs/2106.15110">Time-Aware Language Models as Temporal Knowledge Bases</a> in preparation.
</li>
<li>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov (2021): <a href="https://arxiv.org/abs/2106.00200">End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents</a> in preparation.<br><ul><li><font size=-1>Adapts many of the ideas used for multihop KBQA to a new task - answering multihop questions over a large document.  Retrieval steps in this "DocHopper" system retrieve passages of a document, and the retrieved items are combined with a question neurally: i.e., rather than appending text to a question and re-encoding that discrete object, what is retrieved is a vector summary of the document, which is mixed with the previous question encoding.  This is fast, fully differentiable, allows retrieval of large document subsections, and gets a new SOTA on three datasets.</font></ul>
</li>
<li>Avishai Zagoury, Einat Minkov, Idan Szpektor, William W. Cohen (2021): <a href="https://arxiv.org/abs/2104.01940">What's the best place for an AI conference, Vancouver or ______: Why completing comparative questions is difficult</a> in AAAI2021.
</li>
<li>Haitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan Salakhutdinov, William W. Cohen (2021): <a href="https://arxiv.org/abs/2102.07043">Reasoning Over Virtual Knowledge Bases With Open Predicate Relations</a> in ICML2021.<br><ul><li><font size=-1>Modifies the FILM model by using a virtual KB of small text passages containing pairs of entities.  This required adding a Matching-the-Blanks pretraining phase, but got strong results on a number of QA-from-corpora tasks.</font></ul>
</li>
<li>Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, William W. Cohen (2021): <a href="https://openreview.net/forum?id=yRJUNfGzdsT">Open Question Answering Over Tables and Text</a> in ICLR-2021.<br><ul><li><font size=-1>Answering open QA multi-hop questions over tables and text with a clever ``early fusion'' idea, which proposes and indexes likely reasoning chains, and uses large-document Transformers to merge these noisy evidence chains.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2021): <a href="https://aclanthology.org/2021.naacl-main.288/">Adaptable and Interpretable Neural Memory Over Symbolic Knowledge</a> in NAACL-2021.<br><ul><li><font size=-1>Most recent paper on Fact-Injected Language Model (FILM), which includes an Entities-as-Experts style memory of neural entity encodings, plus a second "fact memory" of KG triples.  FILM has good results on KBQA tasks, and allows one to use an edited KB with retraining.</font></ul>
</li>
<li>Danish Pruthi, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen (2020): <a href="https://arxiv.org/abs/2012.00893">Evaluating Explanations: How Much Do Explanations From the Teacher Aid Students?</a> in preparation.
</li>
<li>Bill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang Ren, William W. Cohen (2020): <a href="https://arxiv.org/abs/2010.14439">Differentiable Open-Ended Commonsense Reasoning</a> in NAACL-2021.<br><ul><li><font size=-1>Extends DrKIT's virtual KB to a corpus of documents of common-sense statements ("facts").  In DrFact, entities are replaced by noisy and ambiguous concepts, and navigation is between documents with overlapping sets of mentions.  Also introduces new "open" tasks for common-sense QA.</font></ul>
</li>
<li>Haitian Sun, Andrew O. Arnold, Tania Bedrax-Weiss, Fernando Pereira, William W. Cohen (2020): <a href="https://arxiv.org/abs/2004.03658">Faithful Embeddings for Knowledge Base Queries</a> in NeurIPS2020.<br><ul><li><font size=-1>An extension to Neural Query Language (NQL) which extends the query language to work with a "centroid-sketch" representation of sets.  The centroid encoders a geometric area, and the sketch is a randomized data structure that adds capacity to the sketch, allowing faithful differential logical reasoning to be combined with good generalization.</font></ul>
</li>
<li>Pat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen (2020): <a href="https://arxiv.org/abs/2007.00849">Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge</a> in arxiv.<br><ul><li><font size=-1>Earlier draft of the NAACL paper on FILM (Fact-Injected LM).</font></ul>
</li>
<li>William W. Cohen, Fan Yang, and Kathryn Rivard Mazaitis (2020): <a href="https://jair.org/index.php/jair/article/view/11944/26561">TensorLog: A Probabilistic Database Implemented Using Deep-Learning Infrastructure</a> in JAIR.<br><ul><li><font size=-1>Most complete paper on TensorLog, a predecessor of NQL/EmQL that was a Prolog-like logic, not a dataflow query language.</font></ul>
</li>
<li>William W. Cohen, Haitian Sun, R. Alex Hofer, Matthew Siegler (2020): <a href="https://arxiv.org/abs/2002.06115">Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base </a> in ICLR-2020.<br><ul><li><font size=-1>Paper on Neural Query Language (NQL) a differentiable dataflow query language.  NQL is useful for building KBQA systems that can be trained from denotations, but relies heavily on sparse-matrix operations that are not implemented in all accelerators.</font></ul>
</li>
<li>Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, William W. Cohen (2020): <a href="https://openreview.net/forum?id=SJxstlHFPH">Differentiable Reasoning over a Virtual Knowledge Base</a> in ICLR-2020.<br><ul><li><font size=-1>Describes DrKIT, which allows one to answer multihop chain queries on a "virtual KB"---a corpus of entity-linked documents.  In DrKIT, entity mentions are indexed for neural retrieval with a rich representation of their context, and reasoning consists of navigating between co-occurring mentions.</font></ul>
</li>
<li>Yifeng Tao, Chunhui Cai, William W. Cohen, Xinghua Lu (2020): <a href="postscript/psb-2020.pdf">From genome to phenome: Predicting multiple cancer phenotypes based on somatic genomic alternations bia the genomic impact transformer</a> in PSB-2020.
</li>
<li>Andrew O. Arnold, William W. Cohen (2019): <a href="https://arxiv.org/abs/1911.06111">Instance-based Transfer Learning for Multilingual Deep Retrieval</a> in arxiv.
</li>
<li>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu (2019): <a href="https://arxiv.org/pdf/1909.06146.pdf">PubMedQA: A Dataset for Biomedical Research Question Answering</a> in EMNLP-2019.
</li>
<li>Bhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, William W. Cohen (2019): <a href="http://arxiv.org/abs/1906.01081">Handling Divergent Reference Texts when Evaluating Table-to-Text Generation</a> in ACL-2019.
</li>
<li>William W. Cohen, Haitian Sun, Alex Hofer, Matthew Siegler  (2019): <a href="http://arxiv.org/abs/1905.10417">Differentiable Representations For Multihop Inference Rules</a> in arxiv.<br><ul><li><font size=-1>Earlier version of ICLR paper on NQL.</font></ul>
</li>
<li>William W. Cohen, Matthew Siegler, Alex Hofer (2019): <a href="http://arxiv.org/abs/1905.06209">Neural Query Language: A Knowledge Base Query Language for Tensorflow</a> in arxiv.<br><ul><li><font size=-1>Earlier version of ICLR paper on NQL focusing on the language constructs used.</font></ul>
</li>
<li>Haitian Sun, Tania Bedrax-Weiss, William W. Cohen (2019): <a href="http://arxiv.org/abs/1904.09537">PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text</a> in EMNLP-2019.
</li>
<li>Qiao Jin, Bhuwan Dhingra, William W. Cohen, Xinghua Lu (2019): <a href="http://arxiv.org/abs/1904.02181">Probing Biomedical Embeddings from Language Models</a> in NAACL-2019.
</li>
<li>Haohan Wang, Xiang Liu, Yifeng Tao, Wenting Ye, Qiao Jin, William W. Cohen and Eric P. Xing (2019): <a href="https://www.worldscientific.com/doi/abs/10.1142/9789813279827_0011">Automatic Human-like Mining and Constructing Reliable Genetic Association Database with Deep Reinforcement Learning</a> in Biocomputing.
</li>
<li>Haitian Sun, William W. Cohen, Lidong Bing (2018): <a href="http://arxiv.org/abs/1804.09238">Semi-Supervised Learning with Declaratively Specified Entropy Constraints</a> in NIPS-2018.
</li>
<li>Zhilin Yang, Jake (Junbo) Zhao, Bhuwan Dhingra, Kaiming He, William W. Cohen, Ruslan Salakhutdinov, Yann LeCun (2018): <a href="http://arxiv.org/abs/1806.05662">GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations</a> in NIPS-2018.
</li>
<li>Qiao Jin, Bhuwan Dhingra, William W. Cohen, and Xinghua Lu (2018): <a href="postscript/bioasq-2018.pdf">AttentionMeSH: Simple, Effective and Interpretable Automatic MeSH Indexer</a> in BioASQ-2018.
</li>
<li>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, Christopher D. Manning (2018): <a href="http://arxiv.org/abs/1809.09600">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</a> in EMNLP-2018.
</li>
<li>Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William W. Cohen (2018): <a href="http://arxiv.org/abs/1809.00782">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</a> in EMNLP-2018.
</li>
<li>Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov (2018): <a href="https://www.aclweb.org/anthology/N18-2007/">Neural Models for Reasoning over Multiple Mentions using Coreference</a> in NAACL-2018.
</li>
<li>Vidhisha Balachandran and  Dheeraj Rajagopal and , Rose Catherine Kanjirathinkal and William W. Cohen (2018): <a href="https://www.aclweb.org/anthology/W18-6122/">Learning to Define Terms in the Software Domain</a> in W-NUT 2018.
</li>
<li>Fan Yang, Jiazhong Nie, William W. Cohen, Ni Lao (2017): <a href="http://arxiv.org/abs/1711.06744">Learning to Organize Knowledge with N-Gram Machines</a> in arxiv.org/abs/1711.06744.
</li>
<li>Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen (2017): <a href="http://arxiv.org/abs/1711.03953">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</a> in arxiv.org 1711.03953.
</li>
<li>Fan Yang, Zhilin Yang, William W. Cohen (2017): <a href="postscript/nips-2017-diffrule.pdf">Differentiable Learning of Logical Rules for Knowledge Base Reasoning</a> in NIPS-2017.
</li>
<li>Zihang Dai, Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov (2017): <a href="postscript/nips-2017-badgan.pdf">Good Semi-supervised Learning that Requires a Bad GAN</a> in NIPS-2017.
</li>
<li>William W. Cohen and Fan Yang (2017): <a href="http://arxiv.org/abs/1707.05390">TensorLog: Deep Learning Meets Probabilistic Databases</a> in arxiv.org 1707.05390.
</li>
<li>Rose Catherine, Kathryn Mazaitis, Maxine Eskenazi, William W. Cohen (2017): <a href="http://arxiv.org/abs/1707.05254">Explainable Entity-based Recommendations with Knowledge Graphs (poster paper)</a> in RecSys-2017.
</li>
<li>Bhuwan Dhingra, Kathryn Mazaitis, William W. Cohen (2017): <a href="http://arxiv.org/abs/1707.03904">Quasar: Datasets for Question Answering by Search and Reading</a> in arxiv 1707.03904.
</li>
<li>Bhuwan Dhingra, Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov (2017): <a href="http://arxiv.org/abs/1703.02620">Linguistic Knowledge as Memory for Recurrent Neural Networks</a> in arxiv 1703.02620.
</li>
<li>Bhuwan Dhingra, Hanxiao Liu, Ruslan Salakhutdinov, and William W. Cohen (2017): <a href="http://arxiv.org/abs/1703.00993">A Comparative Study of Word Embeddings for Reading Comprehension</a> in arxiv 1703.00993.
</li>
<li>Rose Catherine, William W. Cohen (2017): <a href="http://arxiv.org/abs/1704.02298">TransNets: Learning to Transform for Recommendation</a> in RecSys-2017.
</li>
<li>Bhuwan Dhingra, Hanxiao Liu, William W. Cohen, and Ruslan Salakhutdinov (2017): <a href="http://arxiv.org/abs/1606.01549">Gated-Attention Readers for Text Comprehension</a> in ACL-2017.
</li>
<li>Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William W. Cohen (2017): <a href="http://arxiv.org/abs/1702.02206">Semi-Supervised QA with Generative Domain-Adaptive Nets</a> in ACL-2017.
</li>
<li>Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, Ruslan Salakhutdinov (2017): <a href="http://arxiv.org/abs/1611.01724">Words or Characters? Fine-grained Gating for Reading Comprehension</a> in ICLR 2017.
</li>
<li>Zhilin Yang, Ruslan Salakhutdinov, William W. Cohen  (2017): <a href="http://arxiv.org/abs/1703.06345">Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks</a> in ICLR 2017.
</li>
<li>William W. Cohen (2016): <a href="http://arxiv.org/abs/1605.06523">TensorLog: A Differentiable Deductive Database</a> in arxiv.org 1605.06523.
</li>
<li>Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W. Cohen (2016): <a href="postscript/nips-2016.pdf">Encode, Review, and Decode: Reviewer Module for Caption Generation</a> in NIPS-2016.
</li>
<li>Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl and William W. Cohen (2016): <a href="postscript/acl-2016-bd.pdf">Tweet2Vec: Character-Based Distributed Representations for Social Media</a> in ACL-2016 (short paper).
</li>
<li>William Yang Wang and William W. Cohen (2016): <a href="postscript/ijcai-2016-yww.pdf">Learning First-Order Logic Embeddings via Matrix Factorization</a> in IJCAI-2016.
</li>
<li>Zhilin Yang, Jei Tang, and William W. Cohen (2016): <a href="postscript/ijcai-2016-zy.pdf">Multi-Modal Bayesian Embeddings for Learning Social Knowledge Graphs</a> in IJCAI-2016.
</li>
<li>Zhilin Yang, Ruslan Salakhutdinov, William Cohen (2016): <a href="http://arxiv.org/abs/1603.08861">Revisiting Semi-Supervised Learning with Graph Embeddings</a> in ICML-2016.
</li>
<li>Zhilin Yang, Ruslan Salakhutdinov, William Cohen (2016): <a href="http://arxiv.org/abs/1603.06270">Multi-Task Cross-Lingual Sequence Tagging from Scratch</a> in arxiv 1603.06270.
</li>
</ol>
<p align="center">[<a href="pubs-s.html">Selected papers</a>| By topic: <a href="pubs-G.html">GNAT System</a>|  <a href="pubs-R.html">Retrieval Augmented LMs</a>|  <a href="pubs-a.html">Applications</a>|  <a href="pubs-c.html">Collaborative Filtering</a>|  <a href="pubs-d.html">Intelligent Tutoring</a>|  <a href="pubs-e.html">Explanation-Based Learning</a>|  <a href="pubs-f.html">Formal Results</a>|  <a href="pubs-g.html">Learning in Graphs</a>|  <a href="pubs-i.html">Inductive Logic Programming</a>|  <a href="pubs-k.html">Neural Knowledge Representation</a>|  <a href="pubs-l.html">Topic Modeling</a>|  <a href="pubs-m.html">Matching/Data Integration</a>|  <a href="pubs-n.html">Deep Learning</a>|  <a href="pubs-r.html">Rule Learning</a>|  <a href="pubs-t.html">Text Categorization</a>|  <a href="pubs-x.html">Info Extraction/Reading/QA</a>|  By year: <a href="pubs.html">All papers</a>]</p>
</body></html>
